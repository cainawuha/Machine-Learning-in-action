import math

#create dataset
def createDataSet():
    dataSet = [[1, 1, 'yes'],
               [1, 1, 'yes'],
               [1, 0, 'no'],
               [0, 1, 'no'],
               [0, 1, 'no']]
    featureNames = ['no surfacing','flippers'] #
    #change to discrete values
    return dataSet, featureNames

#calculayte entropy
def Entropy(dataSet):
    num = len(dataSet)
    labelCounts = {}
    for featVec in dataSet: # number of categories
        currentLabel = featVec[-1] # the last column is category
        if currentLabel not in labelCounts.keys(): 
            labelCounts[currentLabel] = 0 #initial value = 0
        labelCounts[currentLabel] += 1 
    entropy = 0.0
    for key in labelCounts:  
        prob = float(labelCounts[key])/num
        entropy -= prob * math.log(prob,2) #log base 2
    return entropy
#take a test
myData,myFeatureNames = createDataSet()
print "the old dataset:\n",myData

myEntropy = Entropy(myData)
print "my test entropy should be 0.97095 :\n",myEntropy



#split dataset
def splitDataSet(dataSet, axis, value):
    returnDataSet = []
    for dataVec in dataSet:
        if dataVec[axis] == value:
            tempVec = dataVec[:axis]     #0--(axis-1)
            tempVec.extend(dataVec[axis+1:]) #(axis+1)--(-1)
            returnDataSet.append(tempVec) 
    return returnDataSet
#take a test
myData,myFeatureNames = createDataSet()
print splitDataSet(myData,0,1) #axis = 0,and value=1


#ChooseBestFeature
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0]) - 1     
    baseEntropy = Entropy(dataSet)         
    bestInfoGain = 0.0; bestFeature = -1
    
    for i in range(numFeatures):        
        featureList = [example[i] for example in dataSet]
        uniqueVal = set(featureList)       #deduplication
        newEntropy = 0.0
        for value in uniqueVal:  
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet)/float(len(dataSet))
            newEntropy += prob * Entropy(subDataSet)    
        infoGain = baseEntropy - newEntropy    
        if (infoGain > bestInfoGain):       
            bestInfoGain = infoGain         
            bestFeature = i
    return bestFeature                     


#take a test
index = chooseBestFeatureToSplit(myData)
print "best feature shoule be 0\n",index


# use pickle to save tree and data structure
import pickle
define storetree(inputtree,filename):
       fw = open (filename,'w')
       pickle.dump(inputtree,fwï¼‰
       fw.close()
       
define gettree(filename):
       fr = open(filename)
       return pickle.load(fr)
