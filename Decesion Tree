#决策树算法

import math
#小的数据集
def createDataSet():
    dataSet = [[1, 1, 'yes'],
               [1, 1, 'yes'],
               [1, 0, 'no'],
               [0, 1, 'no'],
               [0, 1, 'no']]
    featureNames = ['no surfacing','flippers'] #不浮出水面是否存活 ，有无脚蹼
    #change to discrete values
    return dataSet, featureNames

#计算信息熵,因为我们会利用最大信息增益的方法划分数据集-----看哪个特征划分使得，信息熵(数据无序度)减小的最多
def Entropy(dataSet):
    num = len(dataSet)
    labelCounts = {}
    for featVec in dataSet: # 统计每个类别的数量
        currentLabel = featVec[-1] #最后1列为键
        if currentLabel not in labelCounts.keys(): 
            labelCounts[currentLabel] = 0 #初始值=0
        labelCounts[currentLabel] += 1 #统计+1
    entropy = 0.0
    for key in labelCounts:  #
        prob = float(labelCounts[key])/num
        entropy -= prob * math.log(prob,2) #log base 2
    return entropy

#测试一下
myData,myFeatureNames = createDataSet()
print "the old dataset:\n",myData

myEntropy = Entropy(myData)
print "my test entropy should be 0.97095 :\n",myEntropy

#添加一类，mabey,yes,no   ====熵越高，表明数据越混乱
myData[0][-1] = "mabey"
print "the new dataset:\n",myData
myEntropy = Entropy(myData)
print "my test entropy should be 1.37095 :\n",myEntropy
